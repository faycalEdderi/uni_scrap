#!/usr/bin/env python3
"""
Script de d√©marrage rapide pour Fandom Explorer
Usage: python start_project.py [--scraper-only] [--frontend-only]
"""

import os
import sys
import subprocess
import argparse
import webbrowser
from pathlib import Path


def check_requirements():
    """V√©rifie que les pr√©requis sont install√©s"""
    print("üîç V√©rification des pr√©requis...")
    
    # V√©rifier Python
    if sys.version_info < (3, 8):
        print("‚ùå Python 3.8+ requis")
        return False
    
    print("‚úÖ Python OK")
    
    # V√©rifier Node.js (avec support Windows)
    npm_cmd = 'npm.cmd' if os.name == 'nt' else 'npm'
    node_cmd = 'node.exe' if os.name == 'nt' else 'node'
    
    try:
        result = subprocess.run([node_cmd, '--version'], capture_output=True, text=True)
        if result.returncode == 0:
            print("‚úÖ Node.js OK")
        else:
            print("‚ùå Node.js non trouv√©")
            return False
    except FileNotFoundError:
        print("‚ùå Node.js non install√©")
        return False
    
    return True


def setup_scraper():
    """Configure et d√©marre l'environnement scraper"""
    print("\nüêç Configuration du scraper...")
    
    scraper_dir = Path("scraper")
    if not scraper_dir.exists():
        print("‚ùå Dossier scraper non trouv√©")
        return False
    
    os.chdir(scraper_dir)
    
    # V√©rifier que les d√©pendances sont install√©es
    try:
        import scrapy
        print("‚úÖ Scrapy install√©")
    except ImportError:
        print("‚ùå Scrapy non install√©. Installez les d√©pendances :")
        print("   pip install -r requirements.txt")
        return False
    
    print("‚úÖ Scraper pr√™t !")
    print("\nüìù Commandes utiles :")
    print("   python run_scraper.py https://leagueoflegends.fandom.com/")
    print("   python test_fandoms.py")
    
    os.chdir("..")
    return True


def setup_frontend():
    """Configure et d√©marre le frontend React"""
    print("\n‚öõÔ∏è Configuration du frontend...")
    
    frontend_dir = Path("frontend")
    if not frontend_dir.exists():
        print("‚ùå Dossier frontend non trouv√©")
        return False
    
    os.chdir(frontend_dir)
    
    # Commande npm selon l'OS
    npm_cmd = 'npm.cmd' if os.name == 'nt' else 'npm'
    
    # V√©rifier si node_modules existe
    node_modules = Path("node_modules")
    if not node_modules.exists():
        print("üì¶ Installation des d√©pendances npm...")
        try:
            subprocess.run([npm_cmd, 'install'], check=True)
            print("‚úÖ D√©pendances install√©es")
        except subprocess.CalledProcessError:
            print("‚ùå Erreur lors de l'installation npm")
            os.chdir("..")
            return False
    else:
        print("‚úÖ D√©pendances npm OK")
    
    print("üöÄ D√©marrage du serveur de d√©veloppement...")
    print("   L'application sera accessible sur http://localhost:3000")
    
    # Ouvrir le navigateur apr√®s un d√©lai
    import threading
    def open_browser():
        import time
        time.sleep(3)  # Attendre que le serveur d√©marre
        webbrowser.open('http://localhost:3000')
    
    threading.Thread(target=open_browser, daemon=True).start()
    
    # D√©marrer le serveur React
    try:
        subprocess.run([npm_cmd, 'start'], check=True)
    except KeyboardInterrupt:
        print("\nüõë Serveur arr√™t√©")
    except subprocess.CalledProcessError:
        print("‚ùå Erreur lors du d√©marrage du serveur")
        os.chdir("..")
        return False
    
    os.chdir("..")
    return True


def show_project_info():
    """Affiche les informations du projet"""
    print("""
üé≠ FANDOM EXPLORER
==================

Un scraper universel pour les wikis Fandom avec interface web moderne.

üìÅ Structure:
   scraper/     - Code Python/Scrapy pour le scraping
   frontend/    - Interface React pour visualiser les donn√©es
   data/        - Donn√©es extraites (JSON)

üöÄ D√©marrage rapide:
   1. Lancer le frontend: python start_project.py --frontend-only
   2. Scraper un fandom: cd scraper && python run_scraper.py <URL>
   3. Voir les r√©sultats dans l'interface web

üìö Documentation compl√®te dans README.md
""")


def main():
    parser = argparse.ArgumentParser(description="D√©marre Fandom Explorer")
    parser.add_argument('--scraper-only', action='store_true', help='Configure uniquement le scraper')
    parser.add_argument('--frontend-only', action='store_true', help='D√©marre uniquement le frontend')
    parser.add_argument('--info', action='store_true', help='Affiche les infos du projet')
    
    args = parser.parse_args()
    
    if args.info:
        show_project_info()
        return
    
    print("üé≠ Fandom Explorer - D√©marrage")
    print("=" * 40)
    
    if not check_requirements():
        print("\n‚ùå Pr√©requis manquants. Veuillez les installer avant de continuer.")
        return
    
    if args.scraper_only:
        setup_scraper()
    elif args.frontend_only:
        setup_frontend()
    else:
        # Par d√©faut, configurer le scraper puis d√©marrer le frontend
        if setup_scraper():
            print("\n" + "="*50)
            setup_frontend()


if __name__ == "__main__":
    main()
